{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WILjHzkxbO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a90c2b2-71b7-4232-d009-6035a8e397f4"
      },
      "source": [
        "!pip install --upgrade torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import model_zoo\n",
        "from torchvision.models import vgg\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qokTv2apxy8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "961f5742-c46f-40e4-f5d5-e339b019fc06"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9m7OZ0fy4bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageHandler:\n",
        "\n",
        "    def __init__(self, image_size, content_image_path, style_image_path, device, preserve_colors=False):\n",
        "        self.image_size = image_size\n",
        "        self.device = device\n",
        "        get_color_from_image_path = content_image_path if preserve_colors else None\n",
        "\n",
        "        self.content_image = self.image_loader(content_image_path)\n",
        "        self.style_image = self.image_loader(style_image_path, get_color_from_image_path)\n",
        "        assert self.content_image.size() == self.style_image.size(), \"The content and style image must be of the same size\"\n",
        "\n",
        "    def image_loader(self, image_path, get_color_from_image_path=None):\n",
        "        loader = transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            MatchColorHistogram(get_color_from_image_path),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        image = Image.open(image_path)\n",
        "        image = loader(image).unsqueeze(0)\n",
        "        return image.to(self.device, torch.float)\n",
        "\n",
        "    def image_unloader(self, tensor):\n",
        "        unloader = transforms.Compose([\n",
        "            transforms.ToPILImage()\n",
        "        ])\n",
        "        image = tensor.cpu().clone()\n",
        "        image = image.squeeze(0)\n",
        "        return unloader(image)\n",
        "\n",
        "    def imshow(self, tensor, title=None):\n",
        "        image = self.image_unloader(tensor)\n",
        "        plt.imshow(image)\n",
        "        if title is not None:\n",
        "            plt.title(title)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flVUDwTyy5rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatchColorHistogram(object):\n",
        "\n",
        "    def __init__(self, color_from_image_path):\n",
        "        self.color_from_image_path = color_from_image_path\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.color_from_image_path is None:\n",
        "            return image\n",
        "        source_image = np.asarray(Image.open(self.color_from_image_path))/255.\n",
        "        target_image = np.asarray(image)/255.\n",
        "\n",
        "        mu_target = target_image.mean(axis=(0, 1))\n",
        "        t = target_image - mu_target\n",
        "        t = t.transpose(2, 0, 1).reshape(3, -1)\n",
        "        sigma_target = t.dot(t.T) / t.shape[1]\n",
        "\n",
        "        mu_source = source_image.mean(axis=(0, 1))\n",
        "        s = source_image - mu_source\n",
        "        s = s.transpose(2, 0, 1).reshape(3, -1)\n",
        "        sigma_source = s.dot(s.T) / s.shape[1]\n",
        "\n",
        "        chol_t = np.linalg.cholesky(sigma_target)\n",
        "        chol_s = np.linalg.cholesky(sigma_source)\n",
        "        ts = chol_s.dot(np.linalg.inv(chol_t)).dot(t)\n",
        "\n",
        "        color_transferred_image = ts.reshape(*target_image.transpose(2, 0, 1).shape).transpose(1, 2, 0)\n",
        "        color_transferred_image += mu_source\n",
        "        color_transferred_image *= 255.\n",
        "        color_transferred_image = np.clip(color_transferred_image, 0, 255)\n",
        "        return Image.fromarray(color_transferred_image.astype('uint8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBsCZp7Vy9ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainStyleImageHandler:\n",
        "\n",
        "    def __init__(self, image_size, style_image_path, dataset_path, batch_size, device,):\n",
        "        self.image_size = image_size\n",
        "        self.device = device\n",
        "\n",
        "        train_dataset = datasets.ImageFolder(dataset_path, self.loader)\n",
        "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "        self.style_image = self.style_image_loader(style_image_path, batch_size)\n",
        "\n",
        "    @property\n",
        "    def loader(self):\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.CenterCrop(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def style_image_loader(self, style_image_path, batch_size):\n",
        "        style_image = Image.open(style_image_path)\n",
        "        style_image = self.loader(style_image)\n",
        "        style_image = style_image.repeat(batch_size, 1, 1, 1)\n",
        "        return style_image.to(self.device, torch.float)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laZOD89bzC_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_last_used_conv_layer(layer_names):\n",
        "    layers = []\n",
        "    for name in layer_names:\n",
        "        a, b = name.split('_')[1:]\n",
        "        layers.append((int(a), int(b)))\n",
        "    return 'conv_%s_%s' % sorted(layers)[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsdt_GANyR6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvPadded(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvPadded, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgdNV6SsyRwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Residual(nn.Module):\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv_1 = ConvPadded(channels, channels, kernel_size=3, stride=1)\n",
        "        self.instance_norm_1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv_2 = ConvPadded(channels, channels, kernel_size=3, stride=1)\n",
        "        self.instance_norm_2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.instance_norm_1(self.conv_1(x)))\n",
        "        out = self.instance_norm_2(self.conv_2(out))\n",
        "        return out + x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvhxyDFOyRie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UpsampleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample):\n",
        "        super(UpsampleConv, self).__init__()\n",
        "        self.upsample_layer = nn.Upsample(mode='nearest', scale_factor=upsample)\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.upsample_layer(x)\n",
        "        out = self.reflection_pad(out)\n",
        "        return self.conv2d(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZKJNSmyyWOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EgZw2y4ylpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TransformNetwork, self).__init__()\n",
        "        # initial layers\n",
        "        self.conv_1 = ConvPadded(3, 32, kernel_size=9, stride=1)\n",
        "        self.instance_norm_1 = nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv_2 = ConvPadded(32, 64, kernel_size=3, stride=2)\n",
        "        self.instance_norm_2 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv_3 = ConvPadded(64, 128, kernel_size=3, stride=2)\n",
        "        self.instance_norm_3 = nn.InstanceNorm2d(128, affine=True)\n",
        "        # residual layers\n",
        "        self.residual_1 = Residual(128)\n",
        "        self.residual_2 = Residual(128)\n",
        "        self.residual_3 = Residual(128)\n",
        "        self.residual_4 = Residual(128)\n",
        "        self.residual_5 = Residual(128)\n",
        "        # upsampling layers\n",
        "        self.deconv_1 = UpsampleConv(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.instance_norm_4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv_2 = UpsampleConv(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.instance_norm_5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv_3 = ConvPadded(32, 3, kernel_size=9, stride=1)\n",
        "\n",
        "        # non linear layer\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.instance_norm_1(self.conv_1(x)))\n",
        "        out = self.relu(self.instance_norm_2(self.conv_2(out)))\n",
        "        out = self.relu(self.instance_norm_3(self.conv_3(out)))\n",
        "\n",
        "        out = self.residual_1(out)\n",
        "        out = self.residual_2(out)\n",
        "        out = self.residual_3(out)\n",
        "        out = self.residual_4(out)\n",
        "        out = self.residual_5(out)\n",
        "\n",
        "        out = self.relu(self.instance_norm_4(self.deconv_1(out)))\n",
        "        out = self.relu(self.instance_norm_5(self.deconv_2(out)))\n",
        "        out = self.deconv_3(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlp-o45gydQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransferVgg(nn.Module):\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        super(TransferVgg, self).__init__()\n",
        "        self.layer_names = []\n",
        "        for layer_name, layer in layers:\n",
        "            setattr(self, layer_name, layer)\n",
        "            self.layer_names.append(layer_name)\n",
        "\n",
        "    def forward(self, x, out_keys):\n",
        "        out = {}\n",
        "        for layer_name in self.layer_names:\n",
        "            layer = getattr(self, layer_name)\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                out[layer_name] = F.relu(layer(x))\n",
        "            else:\n",
        "                out[layer_name] = layer(x)\n",
        "            x = out[layer_name]\n",
        "        return {key: value for key, value in out.items() if key in out_keys}\n",
        "\n",
        "    @classmethod\n",
        "    def factory(cls, cfg, last_layer, mean, std):\n",
        "        layers = [('normalization', Normalization(mean, std))]\n",
        "        in_channels = 3\n",
        "        conv_layer = 1\n",
        "        conv_layer_number = 1\n",
        "        pool_layer_number = 1\n",
        "        for v in cfg:\n",
        "            if v == 'M':\n",
        "                layers.append(('pool_%s' % pool_layer_number, nn.MaxPool2d(kernel_size=2, stride=2)))\n",
        "                pool_layer_number += 1\n",
        "                conv_layer += 1\n",
        "                conv_layer_number = 1\n",
        "            else:\n",
        "                conv_layer_name = 'conv_%s_%s' % (conv_layer, conv_layer_number)\n",
        "                layers.append((conv_layer_name, nn.Conv2d(in_channels,\n",
        "                                                          v,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1)))\n",
        "                in_channels = v\n",
        "                if conv_layer_name == last_layer:\n",
        "                    break\n",
        "                conv_layer_number += 1\n",
        "        return cls(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuWUNPCyydFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transfer_vgg19(last_layer, device):\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        model = TransferVgg.factory(cfg=vgg.cfgs['E'],\n",
        "                                    last_layer=last_layer,\n",
        "                                    mean=mean,\n",
        "                                    std=std)\n",
        "        \n",
        "        print(model)\n",
        "\n",
        "        state_dict = model_zoo.load_url(vgg.model_urls['vgg19'], model_dir=\"/content/gdrive/My Drive\", progress=True)\n",
        "\n",
        "        mapping = {\n",
        "            'conv_1_1': 'features.0',\n",
        "            'conv_1_2': 'features.2',\n",
        "            'conv_2_1': 'features.5',\n",
        "            'conv_2_2': 'features.7',\n",
        "            'conv_3_1': 'features.10',\n",
        "            'conv_3_2': 'features.12',\n",
        "            'conv_3_3': 'features.14',\n",
        "            'conv_3_4': 'features.16',\n",
        "            'conv_4_1': 'features.19',\n",
        "            'conv_4_2': 'features.21',\n",
        "            'conv_4_3': 'features.23',\n",
        "            'conv_4_4': 'features.25',\n",
        "            'conv_5_1': 'features.28',\n",
        "            'conv_5_2': 'features.30',\n",
        "            'conv_5_3': 'features.32',\n",
        "            'conv_5_4': 'features.34',\n",
        "        }\n",
        "\n",
        "        new_state_dict = {}\n",
        "        for key, map_to in mapping.items():\n",
        "            new_state_dict['%s.weight' % key] = state_dict['%s.weight' % map_to]\n",
        "            new_state_dict['%s.bias' % key] = state_dict['%s.bias' % map_to]\n",
        "            if key == last_layer:\n",
        "                break\n",
        "        model.load_state_dict(new_state_dict)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        return model.to(device).eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HERsRfrlxyyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return self.weight * F.mse_loss(input, target)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc6EmoFYxyjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        G_input = self.gram_matrix(input)\n",
        "        G_target = self.gram_matrix(target).detach()\n",
        "        return self.weight * F.mse_loss(G_input, G_target)\n",
        "\n",
        "    def gram_matrix(self, input):\n",
        "        a, b, c, d = input.size()\n",
        "        features = input.view(a * b, c * d)\n",
        "        G = torch.mm(features, features.t())\n",
        "        return G.div(a * b * c * d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u61FBf5lx2Vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight):\n",
        "        super(VariationLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, n_channels, image_height, image_width = input.size()\n",
        "        a = input[:, :, :image_height-1, :image_width-1] - input[:, :, 1:, :image_width-1]\n",
        "\n",
        "        b = input[:, :, :image_height-1, :image_width-1] - input[:, :, :image_height-1, 1:]\n",
        "        return self.weight * torch.sum((a**2 + b**2).pow(1.25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JDM7M_jzgVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "155f4ade-379b-45ed-a687-f817e4caa541"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wEsaqaxzgRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "outputId": "5cece1b0-d267-474e-c2a0-3d4c2fbc3628"
      },
      "source": [
        "writer = SummaryWriter('runs')\n",
        "\n",
        "image_handler = ImageHandler(image_size=1200,\n",
        "                                 content_image_path=\"/content/gdrive/My Drive/europe.jpg\",\n",
        "                                 style_image_path=\"/content/gdrive/My Drive/monet.jpg\",\n",
        "                                 device=device,\n",
        "                                 preserve_colors=True)\n",
        "content_layer_names = ['conv_4_2']\n",
        "style_layer_names = ['conv_1_1', 'conv_2_1', 'conv_3_1', 'conv_4_1', 'conv_5_1']\n",
        "layer_names = content_layer_names + style_layer_names\n",
        "\n",
        "last_layer = get_last_used_conv_layer(layer_names)\n",
        "model = transfer_vgg19(last_layer, device)\n",
        "\n",
        "print('Model:' , model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TransferVgg(\n",
            "  (normalization): Normalization()\n",
            "  (conv_1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "Model: TransferVgg(\n",
            "  (normalization): Normalization()\n",
            "  (conv_1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErfPQyhg5rMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_layers = ['conv_4_2']\n",
        "style_layers = ['conv_1_1', 'conv_2_1', 'conv_3_1', 'conv_4_1', 'conv_5_1']\n",
        "\n",
        "content_weights = [float(w) for w in '1'.split(',')]\n",
        "if len(content_weights) == 1:\n",
        "  content_weights = [content_weights[0]] * len(content_layers)\n",
        "else:\n",
        "  assert len(content_weights) == len(content_layers)\n",
        "content_layers_weights = {n: w for n, w in zip(content_layers, content_weights)}\n",
        "\n",
        "style_weights = [float(w) for w in '64000,128000,256000,512000,512000'.split(',')]\n",
        "if len(style_weights) == 1:\n",
        "  style_weights = [style_weights[0]] * len(style_layers)\n",
        "else:\n",
        "  assert len(style_weights) == len(style_layers)\n",
        "style_layers_weights = {n: w for n, w in zip(style_layers, style_weights)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84OwLogtzgLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_features = model(image_handler.content_image, content_layer_names)\n",
        "\n",
        "content_losses = {layer_name: ContentLoss(weight=weight)\n",
        "                    for layer_name, weight in content_layers_weights.items()}\n",
        "\n",
        "style_features = model(image_handler.style_image, style_layer_names)\n",
        "\n",
        "style_losses = {layer_name: StyleLoss(weight=weight)\n",
        "                    for layer_name, weight in style_layers_weights.items()}\n",
        "\n",
        "variation_loss = VariationLoss(weight=0.0001)\n",
        "\n",
        "combination_image = image_handler.content_image.clone()\n",
        "optimizer = optim.LBFGS([combination_image.requires_grad_()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw62a538zf8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0450b294-f4c6-4752-e83c-1b1a0796c96d"
      },
      "source": [
        "run = [0]\n",
        "while run[0] <= 1000:\n",
        "    def closure():\n",
        "        # correct the values of updated input image\n",
        "        combination_image.data.clamp_(0, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(combination_image, layer_names)\n",
        "        variation_score = variation_loss(combination_image)\n",
        "        content_score = torch.sum(torch.stack([loss(out[layer_name], content_features[layer_name].detach())\n",
        "                                                for layer_name, loss in content_losses.items()]))\n",
        "        style_score = torch.sum(torch.stack([loss(out[layer_name], style_features[layer_name].detach())\n",
        "                                                for layer_name, loss in style_losses.items()]))\n",
        "\n",
        "        loss = style_score + content_score + variation_score\n",
        "        loss.backward()\n",
        "\n",
        "        run[0] += 1\n",
        "        print(\"run {}:\".format(run))\n",
        "        print('Style Loss : {:4f} Content Loss: {:4f} Variation Loss: {:4f}'.format(style_score.item(), content_score.item(), variation_score.item()))\n",
        "        writer.add_scalar('Content Loss',\n",
        "                        content_score,\n",
        "                        run[0])\n",
        "        writer.add_scalar('Style Loss',\n",
        "                        style_score,\n",
        "                        run[0])\n",
        "        writer.add_scalar('Loss',\n",
        "                        loss,\n",
        "                        run[0])\n",
        "        if run[0] % 50 == 0:\n",
        "          torchvision.utils.save_image(combination_image, \"/content/gdrive/My Drive/{}.jpg\".format(run))\n",
        "          print('Image Saved:' ,\"{}.jpg\".format(run))\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "combination_image.data.clamp_(0, 1)\n",
        "\n",
        "torchvision.utils.save_image(combination_image, \"{}.jpg\".format(\"Final_nst\"))\n",
        "\n",
        "image_handler.image_unloader(combination_image)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run [1]:\n",
            "Style Loss : 8.713331 Content Loss: 0.000000 Variation Loss: 1.736881\n",
            "run [2]:\n",
            "Style Loss : 8.712934 Content Loss: 0.000000 Variation Loss: 1.736880\n",
            "run [3]:\n",
            "Style Loss : 6.134307 Content Loss: 0.448657 Variation Loss: 1.729397\n",
            "run [4]:\n",
            "Style Loss : 4.697243 Content Loss: 0.570213 Variation Loss: 1.722768\n",
            "run [5]:\n",
            "Style Loss : 2.993133 Content Loss: 0.967437 Variation Loss: 1.709799\n",
            "run [6]:\n",
            "Style Loss : 2.445093 Content Loss: 0.803161 Variation Loss: 1.683814\n",
            "run [7]:\n",
            "Style Loss : 2.046081 Content Loss: 0.750896 Variation Loss: 1.654816\n",
            "run [8]:\n",
            "Style Loss : 1.659144 Content Loss: 0.771045 Variation Loss: 1.613117\n",
            "run [9]:\n",
            "Style Loss : 1.374958 Content Loss: 0.790215 Variation Loss: 1.563882\n",
            "run [10]:\n",
            "Style Loss : 1.207812 Content Loss: 0.796580 Variation Loss: 1.516890\n",
            "run [11]:\n",
            "Style Loss : 1.094532 Content Loss: 0.756679 Variation Loss: 1.485328\n",
            "run [12]:\n",
            "Style Loss : 0.972613 Content Loss: 0.748408 Variation Loss: 1.441691\n",
            "run [13]:\n",
            "Style Loss : 0.881341 Content Loss: 0.735666 Variation Loss: 1.372485\n",
            "run [14]:\n",
            "Style Loss : 0.799922 Content Loss: 0.739761 Variation Loss: 1.325046\n",
            "run [15]:\n",
            "Style Loss : 0.775043 Content Loss: 0.690470 Variation Loss: 1.307475\n",
            "run [16]:\n",
            "Style Loss : 0.716938 Content Loss: 0.691560 Variation Loss: 1.277662\n",
            "run [17]:\n",
            "Style Loss : 0.670669 Content Loss: 0.681079 Variation Loss: 1.241284\n",
            "run [18]:\n",
            "Style Loss : 0.634782 Content Loss: 0.659711 Variation Loss: 1.206299\n",
            "run [19]:\n",
            "Style Loss : 0.599456 Content Loss: 0.647956 Variation Loss: 1.164636\n",
            "run [20]:\n",
            "Style Loss : 0.580319 Content Loss: 0.635691 Variation Loss: 1.123509\n",
            "run [21]:\n",
            "Style Loss : 0.554852 Content Loss: 0.619418 Variation Loss: 1.099829\n",
            "run [22]:\n",
            "Style Loss : 0.544894 Content Loss: 0.616643 Variation Loss: 1.063819\n",
            "run [23]:\n",
            "Style Loss : 0.524302 Content Loss: 0.601278 Variation Loss: 1.042084\n",
            "run [24]:\n",
            "Style Loss : 0.514255 Content Loss: 0.593219 Variation Loss: 1.012881\n",
            "run [25]:\n",
            "Style Loss : 0.497632 Content Loss: 0.583854 Variation Loss: 0.996115\n",
            "run [26]:\n",
            "Style Loss : 0.493549 Content Loss: 0.572273 Variation Loss: 0.967746\n",
            "run [27]:\n",
            "Style Loss : 0.475962 Content Loss: 0.574728 Variation Loss: 0.943733\n",
            "run [28]:\n",
            "Style Loss : 0.472200 Content Loss: 0.560420 Variation Loss: 0.930087\n",
            "run [29]:\n",
            "Style Loss : 0.482596 Content Loss: 0.558795 Variation Loss: 0.898990\n",
            "run [30]:\n",
            "Style Loss : 0.457992 Content Loss: 0.554489 Variation Loss: 0.884890\n",
            "run [31]:\n",
            "Style Loss : 0.456581 Content Loss: 0.537754 Variation Loss: 0.872494\n",
            "run [32]:\n",
            "Style Loss : 0.455539 Content Loss: 0.529035 Variation Loss: 0.856891\n",
            "run [33]:\n",
            "Style Loss : 0.447123 Content Loss: 0.525838 Variation Loss: 0.838186\n",
            "run [34]:\n",
            "Style Loss : 0.448900 Content Loss: 0.520128 Variation Loss: 0.818589\n",
            "run [35]:\n",
            "Style Loss : 0.438984 Content Loss: 0.513909 Variation Loss: 0.811099\n",
            "run [36]:\n",
            "Style Loss : 0.439932 Content Loss: 0.512444 Variation Loss: 0.792904\n",
            "run [37]:\n",
            "Style Loss : 0.434966 Content Loss: 0.507104 Variation Loss: 0.781862\n",
            "run [38]:\n",
            "Style Loss : 0.431015 Content Loss: 0.502931 Variation Loss: 0.771423\n",
            "run [39]:\n",
            "Style Loss : 0.430660 Content Loss: 0.495511 Variation Loss: 0.764089\n",
            "run [40]:\n",
            "Style Loss : 0.426915 Content Loss: 0.492058 Variation Loss: 0.749631\n",
            "run [41]:\n",
            "Style Loss : 0.430755 Content Loss: 0.487076 Variation Loss: 0.735650\n",
            "run [42]:\n",
            "Style Loss : 0.423642 Content Loss: 0.483912 Variation Loss: 0.730443\n",
            "run [43]:\n",
            "Style Loss : 0.421920 Content Loss: 0.480678 Variation Loss: 0.721869\n",
            "run [44]:\n",
            "Style Loss : 0.417057 Content Loss: 0.480436 Variation Loss: 0.713568\n",
            "run [45]:\n",
            "Style Loss : 0.434276 Content Loss: 0.480520 Variation Loss: 0.694396\n",
            "run [46]:\n",
            "Style Loss : 0.419727 Content Loss: 0.469026 Variation Loss: 0.697610\n",
            "run [47]:\n",
            "Style Loss : 0.414459 Content Loss: 0.467829 Variation Loss: 0.693766\n",
            "run [48]:\n",
            "Style Loss : 0.414598 Content Loss: 0.464544 Variation Loss: 0.685904\n",
            "run [49]:\n",
            "Style Loss : 0.418696 Content Loss: 0.462974 Variation Loss: 0.671222\n",
            "run [50]:\n",
            "Style Loss : 0.415742 Content Loss: 0.460631 Variation Loss: 0.664837\n",
            "Image Saved: [50].jpg\n",
            "run [51]:\n",
            "Style Loss : 0.411791 Content Loss: 0.457758 Variation Loss: 0.661933\n",
            "run [52]:\n",
            "Style Loss : 0.411754 Content Loss: 0.455335 Variation Loss: 0.655275\n",
            "run [53]:\n",
            "Style Loss : 0.410107 Content Loss: 0.453258 Variation Loss: 0.648410\n",
            "run [54]:\n",
            "Style Loss : 0.414119 Content Loss: 0.452007 Variation Loss: 0.637645\n",
            "run [55]:\n",
            "Style Loss : 0.408599 Content Loss: 0.449370 Variation Loss: 0.635303\n",
            "run [56]:\n",
            "Style Loss : 0.408352 Content Loss: 0.446699 Variation Loss: 0.630607\n",
            "run [57]:\n",
            "Style Loss : 0.408037 Content Loss: 0.443191 Variation Loss: 0.626425\n",
            "run [58]:\n",
            "Style Loss : 0.408112 Content Loss: 0.442387 Variation Loss: 0.619607\n",
            "run [59]:\n",
            "Style Loss : 0.406066 Content Loss: 0.441327 Variation Loss: 0.614711\n",
            "run [60]:\n",
            "Style Loss : 0.406616 Content Loss: 0.438421 Variation Loss: 0.609956\n",
            "run [61]:\n",
            "Style Loss : 0.404578 Content Loss: 0.436791 Variation Loss: 0.606705\n",
            "run [62]:\n",
            "Style Loss : 0.406191 Content Loss: 0.434995 Variation Loss: 0.601578\n",
            "run [63]:\n",
            "Style Loss : 0.403725 Content Loss: 0.433537 Variation Loss: 0.598732\n",
            "run [64]:\n",
            "Style Loss : 0.405356 Content Loss: 0.431069 Variation Loss: 0.593220\n",
            "run [65]:\n",
            "Style Loss : 0.402025 Content Loss: 0.430933 Variation Loss: 0.590509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5e9e76ac0ba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mcombination_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    430\u001b[0m                     \u001b[0;31m# the reason we do this: in a stochastic setting,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                     \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-5e9e76ac0ba0>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariation_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 564.00 MiB (GPU 0; 14.73 GiB total capacity; 12.15 GiB already allocated; 247.88 MiB free; 13.70 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    }
  ]
}